# ğŸš Apache Airflow 3.0.4 å¿«é€Ÿå»ºæ§‹å°ˆæ¡ˆ

ä¸€å€‹åŸºæ–¼ Docker çš„ Apache Airflow 3.0.4 å¿«é€Ÿå•Ÿå‹•æ¨¡æ¿ï¼Œå°ˆç‚ºè³‡æ–™æŒ–æ˜å’Œ ETL ç®¡é“é–‹ç™¼è¨­è¨ˆã€‚

## âœ¨ ç‰¹è‰²

- ğŸ³ **Docker å®¹å™¨åŒ–**: ä¸€éµå•Ÿå‹•å®Œæ•´ Airflow ç’°å¢ƒ
- âš¡ **Airflow 3.0.4**: æœ€æ–°ç‰ˆæœ¬ï¼Œæ”¯æ´ TaskFlow API
- ğŸ”§ **é–‹ç®±å³ç”¨**: é é…ç½®é–‹ç™¼ç’°å¢ƒå’Œå·¥å…·
- ğŸ“ **æ™ºèƒ½æ¨¡çµ„ç®¡ç†**: è§£æ±º shared æ¨¡çµ„å°å…¥å•é¡Œ
- ğŸ¯ **GCP Composer ç›¸å®¹**: æ”¯æ´é›²ç«¯éƒ¨ç½²
- ğŸ“‹ **Makefile å·¥å…·**: ç°¡åŒ–é–‹ç™¼æ“ä½œ

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. å…‹éš†å°ˆæ¡ˆ
```bash
git clone <repository-url>
cd airflow
```

### 2. å»ºç«‹å¿…è¦ç›®éŒ„
```bash
mkdir -p ./dags ./logs ./plugins ./config
```

### 3. è¨­ç½®ç’°å¢ƒè®Šæ•¸
```bash
echo -e "AIRFLOW_UID=$(id -u)\nAIRFLOW_GID=0" > .env
```

### 4. ä¸€éµå•Ÿå‹•
```bash
make quickstart
```

### 5. è¨ªå• Web UI
æ‰“é–‹ç€è¦½å™¨è¨ªå•: http://localhost:8080

**è¨»**: ç”±æ–¼å·²è¨­å®š `AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS=true`ï¼Œç„¡éœ€ç™»å…¥å³å¯ä½¿ç”¨æ‰€æœ‰åŠŸèƒ½ã€‚

## ğŸ“‚ å°ˆæ¡ˆçµæ§‹

```
airflow/
â”œâ”€â”€ dags/                    # DAG å®šç¾©
â”‚   â”œâ”€â”€ simple_dag.py        # ç°¡å–®æ¸¬è©¦ DAG
â”‚   â”œâ”€â”€ test_dag.py          # é…ç½®æ¸¬è©¦ DAG
â”‚   â””â”€â”€ shared/              # å…±äº«å·¥å…·æ¨¡çµ„
â”‚       â”œâ”€â”€ config/          # é…ç½®ç®¡ç† (Pydantic)
â”‚       â””â”€â”€ utils/           # å·¥å…·å‡½æ•¸
â”œâ”€â”€ plugins/                 # Airflow æ’ä»¶
â”œâ”€â”€ config/                  # ç’°å¢ƒé…ç½®
â”œâ”€â”€ logs/                    # åŸ·è¡Œæ—¥èªŒ
â”œâ”€â”€ .env                     # ç’°å¢ƒè®Šæ•¸
â”œâ”€â”€ docker-compose.yaml      # Docker æœå‹™
â”œâ”€â”€ Dockerfile              # è‡ªå®šç¾©æ˜ åƒæª”
â”œâ”€â”€ requirements.txt        # Python ä¾è³´
â””â”€â”€ Makefile               # é–‹ç™¼å·¥å…·
```

## ğŸ› ï¸ å¸¸ç”¨æŒ‡ä»¤

### åŸºæœ¬æ“ä½œ
```bash
make up          # å•Ÿå‹•æœå‹™
make down        # åœæ­¢æœå‹™
make restart     # é‡å•Ÿæœå‹™
make logs        # æŸ¥çœ‹æ—¥èªŒ
make shell       # é€²å…¥å®¹å™¨
```

### DAG ç®¡ç†
```bash
make dag-list                        # åˆ—å‡ºæ‰€æœ‰ DAG
make dag-trigger DAG_ID=simple_dag   # è§¸ç™¼ DAG
make dag-test DAG_ID=simple_dag      # æ¸¬è©¦ DAG
```

### ç¶­è­·é™¤éŒ¯
```bash
make status      # æª¢æŸ¥æœå‹™ç‹€æ…‹
make clean       # å®Œå…¨æ¸…ç†é‡ç½®
make rebuild     # é‡å»ºæ˜ åƒæª”ä¸¦é‡å•Ÿ
```

## ğŸ“ DAG é–‹ç™¼ç¯„ä¾‹

ä½¿ç”¨ TaskFlow API å»ºç«‹æ–°çš„ DAGï¼š

```python
from airflow.decorators import dag, task
from datetime import datetime
from shared.config.settings import get_config

@task
def extract_data():
    """è³‡æ–™èƒå–ä»»å‹™"""
    config = get_config()
    print(f"é€£æ¥è³‡æ–™åº«: {config.database.host}")
    return "èƒå–å®Œæˆ"

@task
def transform_data(data):
    """è³‡æ–™è½‰æ›ä»»å‹™"""
    print(f"è½‰æ›è³‡æ–™: {data}")
    return "è½‰æ›å®Œæˆ"

@dag(
    dag_id='my_etl_pipeline',
    start_date=datetime(2025, 1, 1),
    schedule=None,
    tags=['etl', 'example']
)
def my_etl_pipeline():
    """æˆ‘çš„ ETL ç®¡é“"""
    extracted = extract_data()
    transformed = transform_data(extracted)

# å»ºç«‹ DAG å¯¦ä¾‹
my_etl_pipeline()
```

## âš™ï¸ é…ç½®ç®¡ç†

### ç’°å¢ƒè®Šæ•¸é…ç½® (.env)
```bash
# è³‡æ–™åº«è¨­å®š
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=analytics

# ETL è¨­å®š
ETL_BATCH_SIZE=1000
ETL_RETRY_COUNT=3

# ç›£æ§è¨­å®š
MONITORING_ENABLED=true
LOG_LEVEL=INFO
```

### Pydantic é…ç½®é¡åˆ¥
```python
# dags/shared/config/settings.py
from pydantic_settings import BaseSettings
from pydantic import Field

class DatabaseConfig(BaseSettings):
    host: str = Field(default="localhost")
    port: int = Field(default=5432)
    name: str = Field(default="airflow")
    
    class Config:
        env_prefix = "DATABASE_"
        env_file = ".env"

def get_config():
    return DatabaseConfig()
```

## ğŸ”§ Airflow 3.0.4 ç‰¹è‰²

### TaskFlow API
- ä½¿ç”¨ `@task` è£é£¾å™¨æ›¿ä»£å‚³çµ± Operator
- è‡ªå‹•è™•ç† XCom å’Œä»»å‹™ä¾è³´
- æ›´ç°¡æ½”çš„ç¨‹å¼ç¢¼çµæ§‹

### å¾®æœå‹™æ¶æ§‹
- `airflow-apiserver`: API æœå‹™å’Œ Web UI
- `airflow-scheduler`: æ’ç¨‹å™¨
- `airflow-dag-processor`: DAG è™•ç†å™¨
- `airflow-triggerer`: è§¸ç™¼å™¨æœå‹™

### æ¨¡çµ„å°å…¥è§£æ±ºæ–¹æ¡ˆ
- `shared/` æ¨¡çµ„æ”¾åœ¨ `dags/` ç›®éŒ„ä¸‹
- è‡ªå‹•é‡æ–°è¼‰å…¥ï¼Œç„¡éœ€é‡å•Ÿå®¹å™¨
- é¿å… `plugins/` ç›®éŒ„çš„é‡å•Ÿé™åˆ¶

## ğŸš€ éƒ¨ç½²åˆ° GCP Composer

### 1. æº–å‚™éƒ¨ç½²æª”æ¡ˆ
```bash
# æ‰“åŒ… DAGs å’Œ shared æ¨¡çµ„
tar -czf dags.tar.gz dags/

# ä¸Šå‚³ requirements.txt
gsutil cp requirements.txt gs://your-composer-bucket/
```

### 2. éƒ¨ç½²åˆ° Composer
```bash
# å»ºç«‹ Composer ç’°å¢ƒ
gcloud composer environments create my-airflow-env \
    --location=asia-east1 \
    --airflow-version=3.0.4

# ä¸Šå‚³ DAGs
gcloud composer environments storage dags import \
    --environment=my-airflow-env \
    --location=asia-east1 \
    --source=dags/
```

## ğŸ“¦ ä¾è³´ç®¡ç†

### æ–°å¢ Python å¥—ä»¶
1. ç·¨è¼¯ `requirements.txt`
2. é‡å»ºæ˜ åƒæª”: `make rebuild`

## ğŸ¤ è²¢ç»æŒ‡å—

1. Fork å°ˆæ¡ˆ
2. å»ºç«‹åŠŸèƒ½åˆ†æ”¯: `git checkout -b feature/new-feature`
3. æäº¤è®Šæ›´: `git commit -am 'Add new feature'`
4. æ¨é€åˆ†æ”¯: `git push origin feature/new-feature`
5. å»ºç«‹ Pull Request

## ğŸ“„ æˆæ¬Š

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT æˆæ¬Šæ¢æ¬¾ - è©³è¦‹ [LICENSE](LICENSE) æª”æ¡ˆ

## ğŸ†˜ æ”¯æ´

- ğŸ“š [Apache Airflow å®˜æ–¹æ–‡ä»¶](https://airflow.apache.org/docs/)
- ğŸ› [å•é¡Œå›å ±](../../issues)
- ğŸ’¬ [è¨è«–å€](../../discussions)

---

**å¿«é€Ÿé–‹å§‹æŒ‡ä»¤**: `make quickstart` ğŸš€